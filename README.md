This repositoiry is the used code for the master thesis: "Improving Actor Critic and Direct Future Prediction in Minecraft" at the moment of the submission.

# Abstrakt
Advantage Actor critic (A2C) is a widely used reinforcement algorithm and Direct Future prediction has shown good results in the multi mission ViZDoom environment, but the mechanism behind both algorithms is not yet well understood. In this work both algorithms are analyzed how they relate to the function approximation error and tested in a multi mission real-time Minecraft environment. Different settings used by the community for A2C are shown to result in either an overestimation or worse policy in even the simplest of environments. For DFP it is shown, how the prediction of the change in measurement between two states can be simplified to only predict the discounted measurement (DDFP), as is done with the reward in A2C. Based on this the critic of A2C is adjusted to predict the discounted measurements, instead of the discounted reward to get the new model Actor Critic Direct Future Prediction (ACDFP). ACDFP is compared with A2C in the Minecraft environment, where ACDFP achieves equal results to A2C in the single mission experiment, while outperforming A2C in the multi mission experiment. Furthermore it is shown that synchronizing the models is better than asynchronous updates and that early training of models can be sped up through a different initialization of the optimizer RMSProp.